{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92a6c8ef",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "889b5456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14dc3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc2b580",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e6304c",
   "metadata": {},
   "source": [
    "## Intake data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976ebcbc",
   "metadata": {},
   "source": [
    "### Data url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "168d0e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "intake_url = \"https://data.austintexas.gov/resource/wter-evkm.csv?$limit=500000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d73e93",
   "metadata": {},
   "source": [
    "### Data pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "529a77b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes = pd.read_csv(intake_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b009f582",
   "metadata": {},
   "source": [
    "### Data preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1704455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aae140d",
   "metadata": {},
   "source": [
    "## Outcome data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9f04e",
   "metadata": {},
   "source": [
    "### Data url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dc6fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_url = \"https://data.austintexas.gov/resource/9t4d-g238.csv?$limit=500000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914c3f8",
   "metadata": {},
   "source": [
    "### Data pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b074bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcomes = pd.read_csv(outcome_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c012c50",
   "metadata": {},
   "source": [
    "### Data preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a11173",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcomes.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a7a0a",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448c9c52",
   "metadata": {},
   "source": [
    "## Missing values in intakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0949a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in df_outcomes\n",
    "missing_values = df_intakes.isnull().sum()\n",
    "print(\"Missing values in df_intakes:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8366e72",
   "metadata": {},
   "source": [
    "## Missing values in outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53ce188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in df_outcomes\n",
    "missing_values = df_outcomes.isnull().sum()\n",
    "print(\"Missing values in df_outcomes:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb128b6",
   "metadata": {},
   "source": [
    "## Unique values in color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda19d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes['color'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcomes['color'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d541bb",
   "metadata": {},
   "source": [
    "High cardinality exists here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49111f",
   "metadata": {},
   "source": [
    "## Unique values in found_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4bbc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes['found_location'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0cb4b3",
   "metadata": {},
   "source": [
    "High cardinality exists here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3ef654",
   "metadata": {},
   "source": [
    "## Unique values in breed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a31123",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes['breed'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a62ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcomes['breed'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d697c",
   "metadata": {},
   "source": [
    "High cardinality here too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a5488",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93f00ab",
   "metadata": {},
   "source": [
    "## Fix datetime column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a47c49",
   "metadata": {},
   "source": [
    "### Intakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80e323a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes['datetime'] = pd.to_datetime(df_intakes['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b776443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes['monthyear'] = pd.to_datetime(df_intakes['datetime2']).dt.strftime('%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6c913bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes = df_intakes.drop(columns=['datetime2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18213d9e",
   "metadata": {},
   "source": [
    "### Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a1cddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcomes['date_of_birth'] = pd.to_datetime(df_outcomes['date_of_birth'])\n",
    "df_outcomes['monthyear'] = pd.to_datetime(df_outcomes['monthyear'], format='%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f43a1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcomes['datetime'] = pd.to_datetime(df_outcomes['datetime'], format='mixed', utc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d27028",
   "metadata": {},
   "source": [
    "## Dropping null values from both dataframes\n",
    "\n",
    "For **df_intakes**, missing names can be handled later in a binary variable, while, variables like *sex_upon_intake* will need to be omitted from the analysis.\n",
    "\n",
    "For **df_outcomes**, missing values in age and sex will be omitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a731a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes = df_intakes.dropna(subset=['sex_upon_intake'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7150dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcomes = df_outcomes.dropna(subset=['outcome_type', 'sex_upon_outcome', 'age_upon_outcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a3f20c",
   "metadata": {},
   "source": [
    "## Combining both dataframes into one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d67baf",
   "metadata": {},
   "source": [
    "### First, sort by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "463e5571",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes_sorted = df_intakes.sort_values('datetime')\n",
    "df_outcomes_sorted = df_outcomes.sort_values('datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9313570a",
   "metadata": {},
   "source": [
    "### Combine both dataframes into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de030478",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(df_intakes_sorted, df_outcomes_sorted, on='animal_id', suffixes=('_intake', '_outcome'), how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb3df06",
   "metadata": {},
   "source": [
    "## Age variable fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbdf480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_age_to_days(age_str, name_str=None):\n",
    "    \"\"\"Convert age string to numeric days\"\"\"\n",
    "    if pd.isna(age_str):\n",
    "        return None\n",
    "    \n",
    "    age_str = age_str.lower()\n",
    "    \n",
    "    # Handle \"0 years\" case - check if newborn based on name containing \"grams\"\n",
    "    if age_str.strip() == \"0 years\":\n",
    "        if name_str and \"grams\" in str(name_str).lower():\n",
    "            return 0  # Newborn\n",
    "        else:\n",
    "            return None  # Missing age data\n",
    "    \n",
    "    total_days = 0\n",
    "    \n",
    "    # Find all number-unit pairs\n",
    "    patterns = [\n",
    "        (r'(\\d+)\\s*year', 365),\n",
    "        (r'(\\d+)\\s*month', 30),\n",
    "        (r'(\\d+)\\s*week', 7),\n",
    "        (r'(\\d+)\\s*day', 1)\n",
    "    ]\n",
    "    \n",
    "    for pattern, multiplier in patterns:\n",
    "        matches = re.findall(pattern, age_str)\n",
    "        for match in matches:\n",
    "            total_days += int(match) * multiplier\n",
    "    \n",
    "    return total_days if total_days > 0 else None\n",
    "\n",
    "def convert_age_to_years(age_str, name_str=None):\n",
    "    \"\"\"Convert age string to numeric years (decimal)\"\"\"\n",
    "    days = convert_age_to_days(age_str, name_str)\n",
    "    return days / 365 if days is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5721bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes['age_upon_intake_days'] = df_intakes.apply(lambda row: convert_age_to_days(row['age_upon_intake'], row.get('name')), axis=1)\n",
    "df_intakes['age_upon_intake_years'] = df_intakes.apply(lambda row: convert_age_to_years(row['age_upon_intake'], row.get('name')), axis=1)\n",
    "\n",
    "df_outcomes['age_upon_outcome_days'] = df_outcomes.apply(lambda row: convert_age_to_days(row['age_upon_outcome'], row.get('name')), axis=1)\n",
    "df_outcomes['age_upon_outcome_years'] = df_outcomes.apply(lambda row: convert_age_to_years(row['age_upon_outcome'], row.get('name')), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298004a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample conversions:\")\n",
    "test_ages = [\"7 years\", \"2 weeks\", \"5 months\", \"10 days\"]\n",
    "for age in test_ages:\n",
    "    print(f\"{age} -> {convert_age_to_days(age)} days, {convert_age_to_years(age):.2f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f944303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['age_upon_outcome_days'] = combined_df.apply(lambda row: convert_age_to_days(row['age_upon_outcome'], row.get('name')), axis=1)\n",
    "combined_df['age_upon_outcome_years'] = combined_df.apply(lambda row: convert_age_to_years(row['age_upon_outcome'], row.get('name')), axis=1)\n",
    "\n",
    "combined_df['age_upon_intake_days'] = combined_df.apply(lambda row: convert_age_to_days(row['age_upon_intake'], row.get('name')), axis=1)\n",
    "combined_df['age_upon_intake_years'] = combined_df.apply(lambda row: convert_age_to_years(row['age_upon_intake'], row.get('name')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e25d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c81b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcomes.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722524bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b44420",
   "metadata": {},
   "source": [
    "### Dropping null values for age upon intake & age upon outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb6f5906",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes = df_intakes.dropna(subset=['age_upon_intake_days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9eef9a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcomes = df_outcomes.dropna(subset=['age_upon_outcome_days'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba7ba5a",
   "metadata": {},
   "source": [
    "### Dropping age upon intake and age upon outcome columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ebb6171",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes = df_intakes.drop(columns=['age_upon_intake'])\n",
    "df_outcomes = df_outcomes.drop(columns=['age_upon_outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values in \n",
    "\n",
    "# 1. Check for null values in the dataset\n",
    "print(df_intakes.isnull().sum())\n",
    "\n",
    "# 2. Check for null values in the dataset\n",
    "print(df_outcomes.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b8446",
   "metadata": {},
   "source": [
    "## Breed variable fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bd6f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_breed_features(df, breed_column='breed'):\n",
    "    \"\"\"Extract meaningful features from breed information\"\"\"\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Animal type already exists, so skipping that classification\n",
    "    \n",
    "    # Mixed breed indicators\n",
    "    df['is_mixed'] = df[breed_column].str.contains('Mix|/', case=False, na=False)\n",
    "    df['num_breeds'] = df[breed_column].str.count('/') + 1\n",
    "    df['num_breeds'] = df['num_breeds'].where(df['is_mixed'], 1)\n",
    "    \n",
    "    # Size categories (based on common breed patterns)\n",
    "    toy_breeds = ['Chihuahua', 'Yorkshire', 'Toy', 'Maltese', 'Pomeranian', 'Papillon', 'Miniature']\n",
    "    small_breeds = ['Terrier', 'Beagle', 'Cocker', 'Dachshund', 'Corgi', 'Pug', 'Shih Tzu']\n",
    "    large_breeds = ['Retriever', 'Shepherd', 'Mastiff', 'Great Dane', 'Rottweiler', 'Great Pyrenees', 'Newfoundland']\n",
    "    \n",
    "    def categorize_size(breed_str):\n",
    "        if pd.isna(breed_str):\n",
    "            return 'Unknown'\n",
    "        breed_str = str(breed_str)\n",
    "        if any(toy in breed_str for toy in toy_breeds):\n",
    "            return 'Toy'\n",
    "        elif any(large in breed_str for large in large_breeds):\n",
    "            return 'Large'\n",
    "        elif any(small in breed_str for small in small_breeds):\n",
    "            return 'Small'\n",
    "        else:\n",
    "            return 'Medium'\n",
    "    \n",
    "    df['size_category'] = df[breed_column].apply(categorize_size)\n",
    "    \n",
    "    # Working/sport groups\n",
    "    working_breeds = ['Shepherd', 'Cattle Dog', 'Border Collie', 'Australian Kelpie', 'Husky', 'Malamute']\n",
    "    sporting_breeds = ['Retriever', 'Pointer', 'Spaniel', 'Setter', 'Vizsla']\n",
    "    terrier_breeds = ['Terrier']\n",
    "    \n",
    "    df['is_working'] = df[breed_column].str.contains('|'.join(working_breeds), case=False, na=False)\n",
    "    df['is_sporting'] = df[breed_column].str.contains('|'.join(sporting_breeds), case=False, na=False)\n",
    "    df['is_terrier'] = df[breed_column].str.contains('Terrier', case=False, na=False)\n",
    "    \n",
    "    # Popular breed indicators\n",
    "    popular_dog_breeds = ['Labrador', 'Golden Retriever', 'Pit Bull', 'Chihuahua', 'Beagle', 'German Shepherd']\n",
    "    popular_cat_breeds = ['Domestic Shorthair', 'Domestic Longhair', 'Siamese']\n",
    "    \n",
    "    df['is_popular_dog'] = df[breed_column].str.contains('|'.join(popular_dog_breeds), case=False, na=False)\n",
    "    df['is_popular_cat'] = df[breed_column].str.contains('|'.join(popular_cat_breeds), case=False, na=False)\n",
    "    \n",
    "    # Exotic/unusual animals\n",
    "    exotic_animals = ['Snake', 'Lizard', 'Turtle', 'Bird', 'Rabbit', 'Ferret', 'Pig', 'Goat', 'Chicken']\n",
    "    df['is_exotic'] = df[breed_column].str.contains('|'.join(exotic_animals), case=False, na=False)\n",
    "    \n",
    "    # Primary breed (for mixed breeds, take the first one)\n",
    "    def extract_primary_breed(breed_str):\n",
    "        if pd.isna(breed_str):\n",
    "            return None\n",
    "        breed_str = str(breed_str)\n",
    "        if '/' in breed_str:\n",
    "            return breed_str.split('/')[0].strip()\n",
    "        elif 'Mix' in breed_str:\n",
    "            return breed_str.replace(' Mix', '').strip()\n",
    "        else:\n",
    "            return breed_str.strip()\n",
    "    \n",
    "    df['primary_breed'] = df[breed_column].apply(extract_primary_breed)\n",
    "    \n",
    "    # Breed complexity score (more complex = more mixed)\n",
    "    df['breed_complexity'] = df['num_breeds'] + df['is_mixed'].astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86188a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = engineer_breed_features(combined_df, 'breed_outcome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6753c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a285902",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.drop(columns=['breed_intake','breed_outcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b435380",
   "metadata": {},
   "source": [
    "## Has name column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2246a",
   "metadata": {},
   "source": [
    "### Fix cases with weight in place of name\n",
    "\n",
    "Some newborn animals have their weight put into the name column. Let's fix this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e9f60e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace weight entries with null values\n",
    "combined_df['name_intake'] = combined_df['name_intake'].replace(r'^\\d+\\s*grams?$', pd.NA, regex=True)\n",
    "combined_df['name_outcome'] = combined_df['name_outcome'].replace(r'^\\d+\\s*grams?$', pd.NA, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d78ff9",
   "metadata": {},
   "source": [
    "### If name at outcome is not null, has_name will be 1, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5f3443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['has_name'] = combined_df['name_outcome'].notnull().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e60a32",
   "metadata": {},
   "source": [
    "## Keeping only Dogs and Cats\n",
    "\n",
    "Less than 6% if instances are not dogs or cats. Thus, for this model, dropping those instances makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98061817",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df[combined_df['animal_type_outcome'].isin(['Dog', 'Cat'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c65618",
   "metadata": {},
   "source": [
    "## Fix found_location column to extract feature\n",
    "\n",
    "The good thing here is that the location an animal is found is noted down, but, due to high cardinality, this feature also needs to be engineered. Capturing the jurisdiction, and area related information is going to be engineered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f724a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_location_features(df, location_column='found_location'):\n",
    "    \"\"\"Extract meaningful features from found_location\"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract jurisdiction/city from parentheses or end of string\n",
    "    def extract_jurisdiction(location_str):\n",
    "        if pd.isna(location_str):\n",
    "            return None\n",
    "        location_str = str(location_str).strip()\n",
    "        \n",
    "        # Pattern for (TX) or (County)\n",
    "        paren_match = re.search(r'\\(([^)]+)\\)', location_str)\n",
    "        if paren_match:\n",
    "            return paren_match.group(1)\n",
    "        \n",
    "        # For cases like \"Manor(TX)\" without space\n",
    "        no_space_match = re.search(r'([A-Za-z]+)\\([^)]+\\)$', location_str)\n",
    "        if no_space_match:\n",
    "            return no_space_match.group(1)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # Extract city/area name\n",
    "    def extract_city(location_str):\n",
    "        if pd.isna(location_str):\n",
    "            return None\n",
    "        location_str = str(location_str).strip()\n",
    "        \n",
    "        # Handle \"Outside jurisdiction\"\n",
    "        if 'outside jurisdiction' in location_str.lower():\n",
    "            return 'Outside Jurisdiction'\n",
    "        \n",
    "        # Extract city before \" in \" or before parentheses\n",
    "        if ' in ' in location_str:\n",
    "            parts = location_str.split(' in ')\n",
    "            if len(parts) > 1:\n",
    "                city_part = parts[1].split('(')[0].strip()\n",
    "                return city_part\n",
    "        \n",
    "        # For direct formats like \"Austin (TX)\" or \"Manor(TX)\"\n",
    "        city_match = re.search(r'^([A-Za-z\\s]+)(?:\\s*\\(|$)', location_str)\n",
    "        if city_match:\n",
    "            return city_match.group(1).strip()\n",
    "        \n",
    "        return location_str\n",
    "    \n",
    "    # Apply extractions\n",
    "    df['jurisdiction'] = df[location_column].apply(extract_jurisdiction)\n",
    "    df['city_area'] = df[location_column].apply(extract_city)\n",
    "    \n",
    "    # Austin area classifications\n",
    "    austin_areas = ['Austin', 'Travis', 'Manor', 'Pflugerville', 'Cedar Park', 'Round Rock', \n",
    "                   'Lakeway', 'Bee Cave', 'West Lake Hills', 'Rollingwood', 'Sunset Valley']\n",
    "    \n",
    "    surrounding_areas = ['Williamson', 'Hays', 'Bastrop', 'Caldwell', 'Georgetown', \n",
    "                        'Leander', 'Cedar Creek', 'Elgin', 'Dripping Springs']\n",
    "    \n",
    "    df['is_austin_metro'] = df['city_area'].isin(austin_areas)\n",
    "    df['is_surrounding_area'] = df['city_area'].isin(surrounding_areas)\n",
    "    df['is_outside_jurisdiction'] = df['city_area'] == 'Outside Jurisdiction'\n",
    "    \n",
    "    # Core Austin vs suburbs\n",
    "    df['is_core_austin'] = df['city_area'] == 'Austin'\n",
    "    df['is_travis_county'] = df['city_area'].isin(['Austin', 'Travis', 'Manor'])\n",
    "    \n",
    "    # Distance categories (approximate)\n",
    "    def categorize_distance(city):\n",
    "        if pd.isna(city):\n",
    "            return 'Unknown'\n",
    "        if city in ['Austin', 'Travis']:\n",
    "            return 'Core'\n",
    "        elif city in ['Manor', 'Pflugerville', 'West Lake Hills', 'Rollingwood', 'Sunset Valley']:\n",
    "            return 'Close Suburbs'\n",
    "        elif city in ['Cedar Park', 'Round Rock', 'Lakeway', 'Bee Cave']:\n",
    "            return 'Far Suburbs'\n",
    "        elif city == 'Outside Jurisdiction':\n",
    "            return 'Outside'\n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    df['distance_category'] = df['city_area'].apply(categorize_distance)\n",
    "    \n",
    "    # Has specific address (contains street info)\n",
    "    # df['has_address_detail'] = df[location_column].str.contains(r'\\d+\\s+\\w+', na=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8625d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = engineer_location_features(combined_df, 'found_location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "886305bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_location_summary(df):\n",
    "    \"\"\"Print summary of location features\"\"\"\n",
    "    print(\"Location Feature Summary:\")\n",
    "    print(f\"Core Austin: {df['is_core_austin'].sum()}\")\n",
    "    print(f\"Travis County: {df['is_travis_county'].sum()}\")\n",
    "    print(f\"Austin Metro: {df['is_austin_metro'].sum()}\")\n",
    "    print(f\"Outside Jurisdiction: {df['is_outside_jurisdiction'].sum()}\")\n",
    "    print(\"\\nDistance categories:\")\n",
    "    print(df['distance_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6130a325",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_location_summary(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139dd496",
   "metadata": {},
   "source": [
    "## Duplicate case fix\n",
    "\n",
    "Duplicates for animal IDs exist because some animals keep returning, and, there are features here to capture these intricacies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "11aec90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return visit feature engineering for combined dataframe\n",
    "combined_df = combined_df.sort_values(['animal_id', 'datetime_intake'])\n",
    "\n",
    "# Visit count for each animal\n",
    "combined_df['visit_count'] = combined_df.groupby('animal_id').cumcount() + 1\n",
    "\n",
    "# Return visit indicators\n",
    "combined_df['is_return_visit'] = (combined_df['visit_count'] > 1).astype(int)\n",
    "combined_df['is_frequent_returner'] = (combined_df['visit_count'] > 2).astype(int)\n",
    "\n",
    "# Days since last visit\n",
    "combined_df['days_since_last_visit'] = combined_df.groupby('animal_id')['datetime_intake'].diff().dt.days\n",
    "combined_df['days_since_last_visit'] = combined_df['days_since_last_visit'].fillna(-1)\n",
    "\n",
    "# Previous outcome type\n",
    "combined_df['previous_outcome_type'] = combined_df.groupby('animal_id')['outcome_type'].shift(1)\n",
    "combined_df['previous_outcome_type'] = combined_df['previous_outcome_type'].fillna('First Visit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc0c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a659b21",
   "metadata": {},
   "source": [
    "## High cardinality check\n",
    "\n",
    "Some columns have high cardinalities, thus, depending on their importance, they will either be trasnformed, or dropped.\n",
    "\n",
    "These columns are:\n",
    "\n",
    "1. name_intake: drop\n",
    "2. name_outcome: drop\n",
    "3. color_intake: drop\n",
    "4. color_outcome: drop\n",
    "5. primary_breed: reduce/drop\n",
    "6. monthyear_intake: split\n",
    "7. age_upon_outcome: drop\n",
    "8. age_upon_intake: drop\n",
    "9. outcome_type: reduce to adoption or no adoption later\n",
    "10. found_location: drop\n",
    "11. city_area: reduce\n",
    "12. outcome_subtype: drop\n",
    "13. animal_id: drop before analysis\n",
    "14. monthyear_outcome: split\n",
    "15. jurisdiction: reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e09e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = combined_df.select_dtypes(include=['object', 'category']).columns\n",
    "cat_unique_counts = combined_df[cat_cols].nunique()\n",
    "print(cat_unique_counts.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4c8f558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop columns\n",
    "columns_to_drop = ['name_intake', 'name_outcome', 'color_intake', 'color_outcome', \n",
    "                   'age_upon_outcome', 'age_upon_intake', 'found_location', \n",
    "                   'outcome_subtype', 'animal_id']\n",
    "combined_df = combined_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# 2. Reduce primary_breed to top 10\n",
    "top_breeds = combined_df['primary_breed'].value_counts().head(10).index\n",
    "combined_df['primary_breed'] = combined_df['primary_breed'].apply(\n",
    "    lambda x: x if x in top_breeds else 'Other'\n",
    ")\n",
    "\n",
    "# 3. Extract month from monthyear_intake\n",
    "combined_df['month_intake'] = pd.to_datetime(combined_df['monthyear_intake']).dt.month\n",
    "combined_df = combined_df.drop(columns=['monthyear_intake'])\n",
    "\n",
    "# 4. Extract month from monthyear_outcome  \n",
    "combined_df['month_outcome'] = pd.to_datetime(combined_df['monthyear_outcome']).dt.month\n",
    "combined_df = combined_df.drop(columns=['monthyear_outcome'])\n",
    "\n",
    "# 5. Reduce city_area to top 10\n",
    "top_cities = combined_df['city_area'].value_counts().head(10).index\n",
    "combined_df['city_area'] = combined_df['city_area'].apply(\n",
    "    lambda x: x if x in top_cities else 'Other'\n",
    ")\n",
    "\n",
    "# 6. Reduce jurisdiction to top 10\n",
    "top_jurisdictions = combined_df['jurisdiction'].value_counts().head(10).index\n",
    "combined_df['jurisdiction'] = combined_df['jurisdiction'].apply(\n",
    "    lambda x: x if x in top_jurisdictions else 'Other'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a859e99",
   "metadata": {},
   "source": [
    "## Create target variable **Is_Adopted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb5190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['outcome_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e193e301",
   "metadata": {},
   "source": [
    "There will be a good class balance doing this, something the Long Beach struggled with. Albeit, this is a simpler analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a8696ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Is_adopted'] = (combined_df['outcome_type'] == 'Adoption').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8c268d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.drop(columns=['outcome_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254e1399",
   "metadata": {},
   "source": [
    "## Deconstructing time related features\n",
    "\n",
    "Since datetime data types don't give enough interpretable information to the ML algorithm, understanding cyclical and temporal patterns would be better to identify with datetime transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5fd6b",
   "metadata": {},
   "source": [
    "### Extract Basic Time Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9612ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For datetime_intake\n",
    "combined_df['intake_year'] = combined_df['datetime_intake'].dt.year\n",
    "combined_df['intake_day'] = combined_df['datetime_intake'].dt.day\n",
    "combined_df['intake_dayofweek'] = combined_df['datetime_intake'].dt.dayofweek\n",
    "\n",
    "# For date_of_birth\n",
    "combined_df['birth_year'] = combined_df['date_of_birth'].dt.year\n",
    "combined_df['birth_month'] = combined_df['date_of_birth'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8150d39d",
   "metadata": {},
   "source": [
    "### Seasonal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ce7d7420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Season (1=Winter, 2=Spring, 3=Summer, 4=Fall)\n",
    "combined_df['intake_season'] = combined_df['month_intake'].map({\n",
    "    12: 1, 1: 1, 2: 1,    # Winter\n",
    "    3: 2, 4: 2, 5: 2,     # Spring\n",
    "    6: 3, 7: 3, 8: 3,     # Summer\n",
    "    9: 4, 10: 4, 11: 4    # Fall\n",
    "})\n",
    "\n",
    "# Is weekend\n",
    "combined_df['intake_is_weekend'] = (combined_df['intake_dayofweek'] >= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ff45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5ad26c",
   "metadata": {},
   "source": [
    "## Removing all other outcome related information\n",
    "\n",
    "Steps taken to prevent data leakage. Some other repetitive columns will also be dropped here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d910803",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.drop(columns=['age_upon_outcome_days', 'age_upon_outcome_years', 'age_upon_intake_years', 'month_outcome', 'datetime_outcome', 'animal_type_outcome', 'sex_upon_outcome', 'datetime_intake'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad74382",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafee9a4",
   "metadata": {},
   "source": [
    "## Drop null values of *age_upon_intake_days*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9afa6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.dropna(subset=['age_upon_intake_days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e91957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad792db",
   "metadata": {},
   "source": [
    "# ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e7151",
   "metadata": {},
   "source": [
    "## Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numerical columns\n",
    "numerical_cols = combined_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Create correlation matrix\n",
    "correlation_matrix = combined_df[numerical_cols].corr()\n",
    "\n",
    "# Create the correlation plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.2f',\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "\n",
    "plt.title('Correlation Matrix of Numerical Variables', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe68f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339378e",
   "metadata": {},
   "source": [
    "## H20 automl run 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f3235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H2O AutoML Modeling\n",
    "\n",
    "# Initialize H2O\n",
    "h2o.init()\n",
    "\n",
    "\"\"\"## Data Preparation for H2O\"\"\"\n",
    "\n",
    "# Create a copy for modeling\n",
    "model_data = combined_df.copy()\n",
    "\n",
    "# Convert categorical columns to strings for H2O\n",
    "categorical_cols = ['animal_type_intake', 'sex_upon_intake', 'intake_type', 'intake_condition',\n",
    "                   'size_category', 'distance_category', 'primary_breed', 'city_area', \n",
    "                   'jurisdiction', 'previous_outcome_type']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in model_data.columns:\n",
    "        model_data[col] = model_data[col].astype(str)\n",
    "\n",
    "# Convert to H2O Frame\n",
    "h2o_data = h2o.H2OFrame(model_data)\n",
    "\n",
    "# Set categorical columns\n",
    "for col in categorical_cols:\n",
    "    if col in h2o_data.columns:\n",
    "        h2o_data[col] = h2o_data[col].asfactor()\n",
    "\n",
    "# Set target as factor for classification\n",
    "h2o_data['Is_adopted'] = h2o_data['Is_adopted'].asfactor()\n",
    "\n",
    "\"\"\"## Train-Test Split\"\"\"\n",
    "\n",
    "train, test = h2o_data.split_frame(ratios=[0.8], seed=42)\n",
    "\n",
    "# Define features and target\n",
    "target = 'Is_adopted'\n",
    "features = [col for col in h2o_data.columns if col != target]\n",
    "\n",
    "print(f\"Training set: {train.nrows} rows\")\n",
    "print(f\"Test set: {test.nrows} rows\")\n",
    "print(f\"Features: {len(features)}\")\n",
    "\n",
    "\"\"\"## H2O AutoML with Specified Algorithms\"\"\"\n",
    "\n",
    "# Define the algorithms we want to include (using correct H2O names)\n",
    "include_algos = [\"XGBoost\", \"GBM\", \"DRF\", \"StackedEnsemble\"]\n",
    "\n",
    "# Run AutoML\n",
    "aml = H2OAutoML(\n",
    "    max_models=50,  # Increase for more hyperparameter combinations\n",
    "    max_runtime_secs=14400,  # 1 hour max runtime\n",
    "    include_algos=include_algos,\n",
    "    seed=42,\n",
    "    balance_classes=True,  # Handle class imbalance\n",
    "    sort_metric=\"AUC\"\n",
    ")\n",
    "\n",
    "# Train the models\n",
    "aml.train(x=features, y=target, training_frame=train, validation_frame=test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d66b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Model Performance Evaluation\"\"\"\n",
    "\n",
    "# View the leaderboard\n",
    "lb = aml.leaderboard\n",
    "print(\"AutoML Leaderboard:\")\n",
    "print(lb.head(rows=10))\n",
    "\n",
    "# Get the best model\n",
    "best_model = aml.leader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7261336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on test set\n",
    "test_performance = best_model.model_performance(test)\n",
    "print(\"\\nBest Model Performance on Test Set:\")\n",
    "print(f\"AUC: {test_performance.auc():.4f}\")\n",
    "print(f\"Accuracy: {test_performance.accuracy()[0][0]:.4f}\")\n",
    "print(f\"Precision: {test_performance.precision()[0][0]:.4f}\")\n",
    "print(f\"Recall: {test_performance.recall()[0][0]:.4f}\")\n",
    "print(f\"F1 Score: {test_performance.F1()[0][0]:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = test_performance.confusion_matrix()\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684306c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a0999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confusion matrix at 0.5 threshold\n",
    "cm_05 = test_performance.confusion_matrix(thresholds=[0.5])\n",
    "print(\"Confusion Matrix at 0.5 threshold:\")\n",
    "print(cm_05)\n",
    "\n",
    "# Get metrics at 0.5 threshold\n",
    "metrics_05 = test_performance.metric('f1', thresholds=[0.5])\n",
    "print(f\"F1 at 0.5 threshold: {metrics_05[0][1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ec3e4",
   "metadata": {},
   "source": [
    "## Best model performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49563a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics at 0.5 threshold\n",
    "cm_05 = test_performance.confusion_matrix(thresholds=[0.5])\n",
    "print(\"Confusion Matrix at 0.5 threshold:\")\n",
    "print(cm_05)\n",
    "\n",
    "# Extract key metrics at 0.5 threshold\n",
    "f1_05 = test_performance.metric('f1', thresholds=[0.5])[0][1]\n",
    "precision_05 = test_performance.metric('precision', thresholds=[0.5])[0][1] \n",
    "recall_05 = test_performance.metric('recall', thresholds=[0.5])[0][1]\n",
    "accuracy_05 = test_performance.metric('accuracy', thresholds=[0.5])[0][1]\n",
    "\n",
    "print(f\"\\nMetrics at 0.5 threshold:\")\n",
    "print(f\"Accuracy: {accuracy_05:.4f}\")\n",
    "print(f\"Precision: {precision_05:.4f}\")\n",
    "print(f\"Recall: {recall_05:.4f}\")\n",
    "print(f\"F1 Score: {f1_05:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb906fb",
   "metadata": {},
   "source": [
    "## Saving model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e79b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get best model\n",
    "best_model = aml.leader\n",
    "print(f\"Best Model: {best_model.model_id}\")\n",
    "print(f\"Algorithm: {best_model.algo}\")\n",
    "\n",
    "# 2. Get hyperparameters\n",
    "print(\"\\nHyperparameters:\")\n",
    "params = best_model.params\n",
    "for key, value in params.items():\n",
    "   if value['actual'] is not None:\n",
    "       print(f\"{key}: {value['actual']}\")\n",
    "\n",
    "# For ensemble, show base models\n",
    "if \"StackedEnsemble\" in best_model.model_id:\n",
    "   print(f\"\\nBase models:\")\n",
    "   for model in best_model.params['base_models']['actual']:\n",
    "       print(f\"- {model}\")\n",
    "\n",
    "# 3. Save model (H2O format - pickle won't work for H2O models)\n",
    "model_path = h2o.save_model(best_model, path=\"./\", force=True)\n",
    "print(f\"\\nModel saved to: {model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64782bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For FastAPI, you'll need to convert to MOJO or use H2O server\n",
    "# Save as MOJO (better for production)\n",
    "mojo_path = best_model.download_mojo(path=\"./\", get_genmodel_jar=True)\n",
    "print(f\"MOJO saved to: {mojo_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72590830",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb1434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b3ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "community-capstone-X1y0cgVF-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
